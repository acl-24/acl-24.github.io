---
title: "CMPUT 503 Exercise 2"
excerpt: "ROS Development and Kinematics <br/><img src='/images/e2D.png'>"
collection: portfolio
---

---

## Code
Code for [excersie 2](https://github.com/phamcnm/ros-phamm/tree/main/exercise-2)


Code to customize and annotate camera image is [here](https://github.com/phamcnm/ros-phamm/blob/main/exercise-2/packages/camera/src/camera_reader_node.py)


Code for the duckiebot to drive forwards and backwards 1.25 meters is [here](https://github.com/phamcnm/ros-phamm/blob/main/exercise-2/packages/drive_back_and_forth/src/drive.py)


Code for duckiebot to rotate is [here](https://github.com/phamcnm/ros-phamm/blob/main/exercise-2/packages/rotate/src/rotate.py)


Code to make the duckiebot travel in a D shape using LEDs to signal state is [here](https://github.com/phamcnm/ros-phamm/blob/main/exercise-2/packages/drive_d/src/drive_d.py)


Code to plot your tracked trajectory from rosbag is [here](https://github.com/phamcnm/ros-phamm/blob/main/exercise-2/packages/drive_d/src/bag.py)


All nodes are shutdown properly in all scripts through the service call:
`rospy.signal_shutdown("Finished")`


Readme updated


## ROS Basics

### Difference between ROS topics, nodes, services, messages, and bags

Nodes in ROS are running processes that perform certain designated tasks. The idea of using nodes in ROS is to reduce coupling between programs and to provide additional fault tolerance in case any nodes fail. Nodes can be combined into a graph and can communicate with each other using topics, which are channels where messages are sent and received by publishers and subscribers. The messages are data structures with pre-specified typed fields. Nodes can publish messages to the ROS topics, and subscribe to topics to continuously receive and process messages in a decoupled manner. Alternatively, nodes can communicate with each other using services, which provide a synchronous request-response mechanism. A ROS node can provide a service for client nodes to send a request and wait to receive information carried by a single reply. The one-to-one relationship between the request message and the reply message defines the service. The messages used in communication between nodes can be recorded into a bag file in both online and offline settings. In ROS, the data carried by bags can be further processed, analyzed and visualized for better understanding of the robot behaviours. 

### Screenshot of subscriber info from “Hello from…”

![alt text](/images/e2pubsub.png)

In this screenshot, it shows two running nodes we implemented - my_publisher_node and my_subscriber_node. The publisher node publishes the string message "Hello from csc22936!" to the topic chatter. The subscriber node subscribes to the same topic and logs "I heard 'Hello from csc22936!'" to the terminal each time a message is received. 

### Screenshot of annotated custom camera image

![alt text](/images/e2camera.png)

To achieve this display, we: (1) created a camera reader node that subscribes to the Duckiebot camera, (2) used cv2 to turn the camera display into gray scale and drew the text 'Duck csc22936 says, 'Cheese! Capturing on the camera's display 480x640 - quack-tastic!' onto the image, (3) published messages with sensor_msgs.msg.Image type to a custom topic named poster (4) chose the custom topic on rqt_image_view to view the final image.

## Move the Duckiebot 

### Video of duckiebot driving forward then backwards

![alt text](/images/e21.25.png)
[Link to Video](https://youtube.com/shorts/M6-FEvGpWSU)

The video shows the duckiebot moving forward for 1.25 m, pausing for 1 second, and backing for 1.25 m to stop close to the starting location. The node executing driving forth and back subscribes to the both right and left wheel encoder topics to retrieve the current tick and determines how far the robot has traveled on each wheel. Based on the estimated position of the duckiebot, the node publishes to the wheels_driver_node/wheels_cmd topic to update the velocity of each wheel.

### Why is there a difference between actual and desired location?
Our results show that when driving straight, the actual vertical distance travelled is accurate. The video also shows that the rotational and curving movements are more unpredictable. The causes are varying, but we suspect that in our case, the main culprits for any movement unpredictability are: (1) uneven flooring, which leads to variable contact pressure, and (2) mechanical wears, such as the misalignment of wheels, or the wearing down of wheel treads.

### What speed did you use?
We used the default gain and trim parameter of the Duckiebot's kinematics. Instead of tuning these parameters for calibration purposes, we scaled the velocity values in the messages published to the wheels. In other words, we used throttle multipliers to scale the published velecity values, achieving caliberation at the same time.

We ran the experiments at both higher speed (higher throttle multipliers) and low speed (lower throttle multipliers). Specifically:

* For high speed, the left throttle is 0.66, the right throttle is 0.7
* For low speed, the left throttle is 0.23, the right throttle is 0.3

### What happens when you increae or decrease the speed?
From our observation, under this approach to speed manipulation, the wheels' velocities do not scale at the same rate. For example, a differential-wheeled robot that can go straight under a certain configuration may not necessarily go straight if both the left and right throttle multipliers are doubled. The cause may be external dynamics (interactions with environments) rather than technical constraints (wheel velocity equations).

Our empirical results show that at higher speeds, the Duckiebot (1) is less affected by slight uneven flooring but (2) is more affected by large bumps, and (3), requires pauses when changing movement direction (for example, in the forth and back drive, we need to pause for one second before going back).


## Turn the Duckiebot

### Video of duckiebot rotating clockwise and back
![alt text](/images/e290.png)
[Link to Video](https://youtube.com/shorts/sWfWMmzjZek)

The video shows the duckiebot rotating clockwise roughly 90 degrees, pausing for 1 second, and moving back to the original orientation. The node responsible for turning also subsribes from the wheel encoder topics to estimate the distance traveled by each wheel and publishes to wheels_driver_node/wheels_cmd topic to update the left and right wheel velocity. During a turning, the velocity of one wheel is unchanged and the velocity of the other wheel is reversed so that the robot ideally stays in the same position after a turn. 

### What could be the cause of deviations in the rotation?

The main causes of deviation could be due to (1) Uneven flooring or unequal friction between contact surfaces, leading to discrepancies between the robot's actual orientation and the calculated value from ticks, (2) Mechanical or hardware issues, such as dust accumulation, motor wear, or wheel misalignment.

## Plotting the tracked trajectory 
![alt text](/images/e2bag.png)



## Duckiebot Pathing 

### Video of the duckiebot driving in a “D” shape
![alt text](/images/e2D.png)
[Link to Video](https://youtube.com/shorts/bexmIOBJRnc)

The video shows the duckiebot driving in the "D" shape tracks and changing its colour between states. The duckiebot performs position calculation and controls wheel velocity using the same method as described above. The total execution time accounting for all three states is 39 seconds.

### LED mapping criteria

* In state 1, the duckiebot starts stationary for 5 seconds. The LED light for this state is purple (rgb: [1, 0, 1]).
* In state 2, the duckiebot is tracing the "D" path. The LED light for this state is green (rgb: [0, 1, 0]).
* In state 3, the duckiebot returns to the starting position, waits for 5 seconds, and changes the LED light. The LED light for this state is blue (rgb: [0, 0, 1]).

## Write Up

---

## References

### Collboration
- The report is a joint effort by Alex Liu and Minh Pham on the completed deliverables of the lab, including the aformentioned screenshots and video, as well as the github repository
- Questions regarding the lab were answered by TAs of the course


### AI Usages
- ChatGPT was used to reword some sentences in this report and to assist in coding

### Resources
- <https://wiki.ros.org>
- <https://note.nkmk.me/en/python-opencv-pillow-image-size>
- <https://www.geeksforgeeks.org/python-grayscaling-of-images-using-opencv/>
- <https://docs.duckietown.com/daffy/devmanual-software/beginner/ros/index.html>
- <https://learnopencv.com/annotating-images-using-opencv/>
- <https://stackoverflow.com/questions/72690021/how-to-process-a-image-message-with-opencv-from-ros2>
- <https://docs-old.duckietown.org/daffy/duckietown-robotics-development/out/odometry_modeling.html>
- <https://docs.duckietown.com/daffy/dt-ros-commons/packages/duckietown_msgs.html>
- <https://community.robotshop.com/forum/t/2-wheel-drift/5636>
