---
title: "CMPUT 503 Exercise 3"
excerpt: "Computer Vision & Controllers for Robotics <br/><img src='/images/e3redline.png'>"
collection: portfolio
---

---

## Code

## Computer Vision

### Lens distortion correction

![alt text](/images/e3distorted.png)
![alt text](/images/e3undistorted.png)

The first image shows the distorted image of the calibration grid caused by lens distortion of the camera. Some curved lines can be spotted for the edges of the objects and the grid lines. The second image shows the undistorted image after undistortion processing using the intrinsic parameters of the camera. The lines of the objects tend to display less of a curvature than the first picture. 

---

### Coloured line detection

![alt text](/images/e3blueline.png)
![alt text](/images/e3redline.png)
![alt text](/images/e3greenline.png)

To detect the coloured lines, the camera topic of the Duckiebot is subscribed and HSV colour thresholds for red, blue, and green are determined. The contours of red, blue, and green colour are captured in real-time in the subscribed camera images and are subsequently labeled and published to be a new topic. The above three images are the screenshots of rqt_image_view subscribing to the post-processing topic when the camera is facing at a green, blue, and red line.  

### How it works

Colour detection works by first defining a lower bound and a upper bound in terms of HSV values. The bounds are turned into a colour mask that is used to find contours with the corresponding colour. The first value in HSV should be selecting the hue of the colour, the second and third value in HSV are for saturation and value, which should be ideally excluding the lower values since those correspond to washed out colours and dark colours. So when tuning HSV parameters for different colours, the hue values should be changed primarily, and the lower bound of the saturation and value can be shifted slightly, while keeping the upper portion of the values relatively the same. 

---

### Line detection behaviours

![alt text](/images/e3.1.1.png)
[Link to Video: Red line behaviour](https://youtube.com/shorts/CKeRNwYd0QA)

![alt text](/images/e3.1.2.png)
[Link to Video: Green line behaviour](https://youtube.com/shorts/NfySnTKrO98)

![alt text](/images/e3.1.3.png)
[Link to Video: Blue line behaviour](https://youtube.com/shorts/gDtSBA5Nm4s)

All three videos show Duckiebot being placed 30-50 cm away from a target line. The Duckiebot captures the colour and the contour of the line while moving forward and approaching the line. When the camera does not observe the line anymore (the Duckiebot is very close to the line), it waits for a short interval and initiates a certain behaviour determined by the colour of the previously observed line:

* Red line behaviour: the Duckiebot moves forward for about 50 cm after the pause.
* Green line behaviour: the Duckiebot signals left LEDs and curve towards the left side after the pause.
* Blue line behaviour: the Duckiebot signals right LEDs and curve towards the right side after the pause.

## Controllers

### Videos of Duckiebot driving using P, PD, PID controllers

![alt text](/images/e3p.png)
[Link to Video: P controller](https://youtube.com/shorts/Skd2RuMp61I)

![alt text](/images/e3pd.png)
[Link to Video: PD controller](https://youtube.com/shorts/tc10usIPpHQ)

![alt text](/images/e3pid.png)
[Link to Video: PID controller](https://youtube.com/shorts/pTgdBer345M)

The above videos show the Duckiebot following a straight line for approximately 1.5 m. The Duckiebot detects the white and the yellow line as the boundaries of the lane using its camera, and attempts to stay between the lines by balancing its camera position from the perceived center of the lane. All three controllers take in the proportion of the camera away from the center of the lane as the proportional error and performs changes on wheel controls according to the degree of being off from the center. However, PD and PID also takes in addtional terms to further control the robot driving.

### Difference between P, PD, and PID controllers

* P controller takes in the proportional error, tuned by changing the coefficient of the proportional error term. 
* PD controller takes in both proportional errors and the rate of change of errors (overshoot), tuned by changing the coefficient of the proportional error term and the derivative error term. 
* PID controller takes in proportional errors, accumulation of past errors (steady-state), and the rate of change of errors, tuned by changing the coefficient of the proportional error term, the derivative error term, and the integral error term. 

A P controller is the simplest to implement, as it relies only on the current error and a single proportional gain. It provides fast responses and works well for basic tasks like following a straight line. However, it cannot eliminate steady-state error, meaning the system may never reach the exact desired value, and it may struggle with more complex trajectories or disturbances. A PD controller builds on the P controller by adding a derivative term, which reacts to the rate of change of error. This makes the controller more responsive to sudden changes, reducing overshoot and improving stability. However, since it lacks an integral term, it cannot correct for accumulated errors over time, making it less effective for tasks requiring long-term precision. A PID controller combines proportional, integral, and derivative actions to address both transient and steady-state performance. The integral term eliminates steady-state error by accumulating past errors, while the derivative term improves system stability by damping oscillations. This makes PID controllers well-suited for handling both dynamic changes and long-term accuracy. However, they require careful tuning to balance responsiveness, stability, and error correction, making them more complex to configure than P or PD controllers.

## Write Up

In this exercise, we learned to develop a fully functional ROS application. We set up DTROS, created a caitkin package, and developed publisher nodes and subscriber nodes that communicate with each other via topics. Specifically, we completed a series of tasks based on wheel encodings of the Duckiebot, such as making the Duckiebot drive straight, curve, rotate, and stop through rospy. We also worked on changing the LED light of the Duckiebot by publishing the messages to the LED emitter node. Through these tasks, we understood ROS basic concepts (such as ROS nodes, topics, and so on) better and learned to apply odometry modeling to hands-on robotics applications and to use rosbag to record and analyze odometry data.

This exercise taught us some practical lessons. First, robotics applications are hard: the results don't always act in accordance with mathematical calculation because of extrernal factors, such as contact surfaces. Secondly, the quality of the hardware is very important for the consistent performance of the Duckiebot. We observed that Duckiebot doesn't always have the left wheel and right wheel activate at the same time, and that phenomenon heavily affects the robot's performance. Finally, proper perception would be very helpful in robotics applications. A big challenge of this exercise is having the robot to perform intended functions through just hard coding based on the data from wheel encoders. But code doesn't adapt to a myriad of uncertainties and scenarios in the real world. So it would be very helpful if the robot can adapt based on its perception of the external world, such as using a camera .

## Bonus

* [Reverse parking](https://youtube.com/shorts/2WlWNV9-NcE)
    * [Code](https://github.com/phamcnm/ros-phamm/blob/main/exercise-2/packages/reverse_park/src/reverse.py)
* [Drive square](https://youtu.be/RDrlkbg8hO0) 
    * [Code](https://github.com/phamcnm/ros-phamm/blob/main/exercise-2/packages/square/src/square.py) 
    * We mentioned above that the accuracy of the wheels are highly affected by external factors, so any shifts in movements would affect the throttle scaling accuracy, straightness in driving, rotations, etc.., and hence the odometry. However, in our experience, this experiment was smooth beside the fact that there was some jerkiness in driving along one side of the square. This jerkiness could be attributed to the robot's reaction.

---

## References

### Collboration
- The report is a joint effort by Alex Liu and Minh Pham on the completed deliverables of the lab, including the aformentioned screenshots and video, as well as the github repository.
- Questions regarding the lab were answered by TAs of the course.

### AI Usages
- ChatGPT and DeepSeek was used to reword some sentences in this report and to assist in coding.

### Resources
- <https://docs.duckietown.com/daffy/opmanual-Duckiebot/operations/calibration_camera/index.html>
- <https://docs.docker.com/reference/cli/docker/container/kill/>
- <https://www.geeksforgeeks.org/multiple-color-detection-in-real-time-using-python-opencv/>

